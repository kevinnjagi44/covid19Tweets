{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid19tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgL_riKtLgV_",
        "outputId": "1c3aaa95-c19d-4c68-a0a7-b323508d2067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "import re\n",
        "import inflect"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0G9NWxIxAk0",
        "outputId": "284e6ed8-563d-4636-8143-ca24dcecc0a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# import training set and test set\n",
        "# encoding latin1 o.w. unicode decode error\n",
        "train_set = pd.read_csv('Corona_NLP_train.csv', encoding='latin1')\n",
        "test_set = pd.read_csv('Corona_NLP_test.csv', encoding='latin1')\n",
        "\n",
        "# drop duplicates from dataframe\n",
        "train_set.drop_duplicates(keep=False,inplace=True) \n",
        "test_set.drop_duplicates(keep=False,inplace=True) \n",
        "\n",
        "train_set.info()\n",
        "\n",
        "# % of missing values per column\n",
        "train_set.isnull().sum()/len(train_set)*100\n",
        "test_set.isnull().sum()/len(train_set)*100"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 8055 entries, 0 to 8054\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   UserName       8055 non-null   int64 \n",
            " 1   ScreenName     8055 non-null   int64 \n",
            " 2   Location       6469 non-null   object\n",
            " 3   TweetAt        8055 non-null   object\n",
            " 4   OriginalTweet  8055 non-null   object\n",
            " 5   Sentiment      8054 non-null   object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 440.5+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UserName          0.000000\n",
              "ScreenName        0.000000\n",
              "Location         10.353818\n",
              "TweetAt           0.000000\n",
              "OriginalTweet     0.000000\n",
              "Sentiment         0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXk6dy_T2sdG",
        "outputId": "90ff4bab-faaf-4013-efb9-f996ae35357e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_set['Sentiment'].value_counts(normalize=True) * 100"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive              26.794140\n",
              "Negative              25.130370\n",
              "Neutral               17.717904\n",
              "Extremely Negative    15.309163\n",
              "Extremely Positive    15.048423\n",
              "Name: Sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHyCj_Bvz1yJ"
      },
      "source": [
        "# preprocessing 'OriginalTweet' column\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def replace_numbers(words):\n",
        "    p = inflect.engine()\n",
        "    text = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new = p.number_to_words(word)\n",
        "            text.append(new)\n",
        "        else:\n",
        "            text.append(word)\n",
        "    return text\n",
        "\n",
        "def preprocess(x):\n",
        "  # remove urls  \n",
        "  x = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE)\n",
        "  # remove https\n",
        "  x = re.sub(r'https?://\\S+', '', x)\n",
        "  # remove noise\n",
        "  x = re.sub('[^a-z\\s]', '', x.lower()) \n",
        "  # remove punctuations\n",
        "  x = re.sub(r'[^\\w\\s]', '', x)\n",
        "  # remove hashtags and mentions\n",
        "  x = x.replace(\"#\", \"\").replace(\"_\", \" \").replace(\"@\", \" \")\n",
        "  # remove stopwords              \n",
        "  x = [w for w in x.split() if w not in stops] \n",
        "  # replace numbers with textual representations\n",
        "  x = replace_numbers(x)\n",
        "  return ' '.join(x)                                   "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqw36vz2V1G",
        "outputId": "7feae455-8005-4764-cc81-caa7b6fa1cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_set['CleanTweet'] = train_set['OriginalTweet'].apply(preprocess) \n",
        "train_set['CleanTweet']"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                             menyrbie philgahan chrisitv\n",
              "1       advice talk neighbours family exchange phone n...\n",
              "2       coronavirus australia woolworths give elderly ...\n",
              "3       food stock one empty please dont panic enough ...\n",
              "4       ready go supermarket covid outbreak im paranoi...\n",
              "                              ...                        \n",
              "8050    hit grocery store early attempt find one elusi...\n",
              "8051    home affairs minister peter dutton says joint ...\n",
              "8052    well never get weightloss side effect anything...\n",
              "8053    nigel balmaingourmet agree wholeheartedly coul...\n",
              "8054    ebay going anything absolutely disgusting pric...\n",
              "Name: CleanTweet, Length: 8055, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxcrvyhQ-VO",
        "outputId": "c1b6db5d-5753-4eba-dcba-a1f9a042434b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# most common words\n",
        "from collections import Counter\n",
        "Counter(\" \".join(train_set[\"CleanTweet\"]).split()).most_common(20)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('covid', 4226),\n",
              " ('coronavirus', 3993),\n",
              " ('store', 1933),\n",
              " ('food', 1748),\n",
              " ('grocery', 1724),\n",
              " ('people', 1319),\n",
              " ('supermarket', 1302),\n",
              " ('prices', 1127),\n",
              " ('amp', 933),\n",
              " ('panic', 910),\n",
              " ('consumer', 866),\n",
              " ('shopping', 836),\n",
              " ('online', 782),\n",
              " ('get', 671),\n",
              " ('need', 655),\n",
              " ('us', 549),\n",
              " ('buying', 544),\n",
              " ('like', 529),\n",
              " ('stock', 490),\n",
              " ('pandemic', 470)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eibEPLPPL6F",
        "outputId": "30e49d8c-4d52-464a-f297-f5ab43ae2846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Ngrams (bigram and trigram)\n",
        "train_set['bigrams'] = train_set['CleanTweet'].apply(lambda row: list(nltk.bigrams(row.split(' '))))\n",
        "train_set['trigrams'] = train_set['CleanTweet'].apply(lambda row: list(nltk.trigrams(row.split(' '))))\n",
        "train_set['bigrams']"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          [(menyrbie, philgahan), (philgahan, chrisitv)]\n",
              "1       [(advice, talk), (talk, neighbours), (neighbou...\n",
              "2       [(coronavirus, australia), (australia, woolwor...\n",
              "3       [(food, stock), (stock, one), (one, empty), (e...\n",
              "4       [(ready, go), (go, supermarket), (supermarket,...\n",
              "                              ...                        \n",
              "8050    [(hit, grocery), (grocery, store), (store, ear...\n",
              "8051    [(home, affairs), (affairs, minister), (minist...\n",
              "8052    [(well, never), (never, get), (get, weightloss...\n",
              "8053    [(nigel, balmaingourmet), (balmaingourmet, agr...\n",
              "8054    [(ebay, going), (going, anything), (anything, ...\n",
              "Name: bigrams, Length: 8055, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2lMj32uQvwE"
      },
      "source": [
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "#word_vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='word')\n",
        "#sparse_matrix = word_vectorizer.fit_transform(train_set['CleanTweet'])\n",
        "#frequencies = sum(sparse_matrix).toarray()[0]\n",
        "#pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])"
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}